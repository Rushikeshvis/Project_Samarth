# -*- coding: utf-8 -*-
"""project_samarth_live.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NeJBjoI_LQaT1bbMqdRr8bp_NhV9NZfz
"""

import streamlit as st
import pandas as pd
import requests
import time
import json
from io import StringIO
import re

class QueryPlanner:
    def __init__(self):
        pass

    def create_plan(self, user_question):
        time.sleep(0.5)
        question_low = user_question.lower()
        search_queries = []

        base_queries = ["rainfall", "crop production", "climate", "agriculture", "agricultural economy", "weather data", "crop yield"]

        for q in base_queries:
            if q in question_low:
                search_queries.append(q)

        states = ["karnataka", "maharashtra", "tamil nadu", "andhra pradesh", "uttar pradesh"]
        for s in states:
            if s in question_low:
                search_queries.append(f"crop production {s}")
                search_queries.append(f"rainfall {s}")

        if not search_queries:
            if "crop" in question_low:
                search_queries.append("crop production")
            if "climate" in question_low or "rainfall" in question_low:
                search_queries.append("annual rainfall")

        if not search_queries:
             search_queries = ["agriculture", "climate"]

        return list(set(search_queries))[:3]

class DataGovAPI:
    CATALOG_RESOURCE_ID = "9ef84268-d588-465a-a308-a864a43d0070"
    BASE_URL = "https://api.data.gov.in"

    def __init__(self, api_key):
        if not api_key:
            raise ValueError("API key is required")
        self.api_key = api_key

    def _make_request(self, url, params):
        try:
            response = requests.get(url, params=params, timeout=20)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.HTTPError as http_err:
            st.error(f"HTTP error occurred: {http_err} - Check your API key or resource ID.")
        except requests.exceptions.ConnectionError as conn_err:
            st.error(f"Connection error occurred: {conn_err}")
        except requests.exceptions.Timeout as timeout_err:
            st.error(f"Timeout error occurred: {timeout_err}")
        except requests.exceptions.RequestException as err:
            st.error(f"An error occurred: {err}")
        except json.JSONDecodeError:
            st.error("Failed to decode JSON response from API.")
        return None

    def search(self, query):
        time.sleep(1)
        search_url = f"{self.BASE_URL}/resource/{self.CATALOG_RESOURCE_ID}"
        params = {
            "api-key": self.api_key,
            "format": "json",
            "filters[title]": query,
            "limit": 5
        }

        data = self._make_request(search_url, params)

        found_datasets = []
        if data and "records" in data:
            for record in data["records"]:
                if record.get("resource_id") and record.get("title"):
                    found_datasets.append({
                        "title": record["title"],
                        "resource_id": record["resource_id"],
                        "ministry": record.get("ministry_department", "N/A"),
                        "description": record.get("desc", "No description provided.")
                    })
        return found_datasets

    def fetch(self, resource_id):
        time.sleep(1.5)
        resource_url = f"{self.BASE_URL}/resource/{resource_id}"

        params_total = {
            "api-key": self.api_key,
            "format": "json",
            "limit": 1
        }

        meta_data = self._make_request(resource_url, params_total)
        if not meta_data or "total" not in meta_data:
            st.warning(f"Could not retrieve total record count for resource {resource_id}.")
            return pd.DataFrame(), {}

        total_records = meta_data.get("total", 10)

        params_all = {
            "api-key": self.api_key,
            "format": "json",
            "limit": total_records
        }

        full_data = self._make_request(resource_url, params_all)

        if full_data and "records" in full_data and len(full_data["records"]) > 0:
            df = pd.DataFrame(full_data["records"])

            metadata = {
                "title": full_data.get("title", resource_id),
                "description": full_data.get("desc", "N/A"),
                "source": full_data.get("source", "data.gov.in"),
                "fields": {f.get("name"): f.get("type") for f in full_data.get("field", [])}
            }
            return df, metadata

        st.warning(f"No records found for resource {resource_id} despite metadata success.")
        return pd.DataFrame(), {}

class LocalLLM:
    def __init__(self):
        self.column_aliases = {
            "state": ["state", "statename", "state_name"],
            "district": ["district", "districtname", "district_name"],
            "year": ["year", "year_code", "financial_year"],
            "crop": ["crop", "crop_name", "commodity"],
            "production": ["production", "production_volume_tonnes", "production_metric_tonnes", "value", "yield"],
            "rainfall": ["rainfall", "avg_annual_rainfall_mm", "actual_rainfall", "rainfall_mm"]
        }

    def _find_col(self, df, col_type):
        for alias in self.column_aliases.get(col_type, []):
            for col in df.columns:
                if alias == col.lower().strip():
                    return col
        return None

    def _clean_numeric(self, series):
        return pd.to_numeric(series.astype(str).str.replace(r'[^\d.]', '', regex=True), errors='coerce')

    def generate(self, user_question, data_context):
        time.sleep(2)

        response = "Based on the retrieved data, I have synthesized the following insights:\n\n"
        citations = []
        question_low = user_question.lower()

        try:
            all_dfs = {}
            for res_id, (df, meta) in data_context.items():
                if not df.empty:
                    all_dfs[res_id] = df
                    citations.append(f"**{meta.get('title', res_id)}** (Source: {res_id}, data.gov.in)")

            if not all_dfs:
                return "I was unable to fetch any data for your query. Please try a different query or check your API key.", []

            if "compare" in question_low and "rainfall" in question_low and "crop" in question_low:
                response += self._synthesize_rainfall_crop_comparison(all_dfs)
            elif "highest" in question_low and "production" in question_low:
                response += self._synthesize_production_leader(all_dfs, question_low)
            elif "trend" in question_low:
                response += self._synthesize_trend(all_dfs, question_low)
            else:
                response += "I found several datasets, but your query was too general to perform a specific analysis. Here is a summary of the data I found:\n\n"
                for res_id, df in all_dfs.items():
                    response += f"**From {citations[list(all_dfs.keys()).index(res_id)]} (first 5 rows):**\n"
                    response += f"{df.head().to_markdown(index=False)}\n\n"

        except Exception as e:
            response = f"I encountered an error during data synthesis, which can be common with inconsistent live data. Error: {e}\n\nI am displaying the raw data I found instead:\n\n"
            for res_id, df in all_dfs.items():
                response += f"**From {citations[list(all_dfs.keys()).index(res_id)]} (first 5 rows):**\n"
                response += f"{df.head().to_markdown(index=False)}\n\n"

        return response, list(set(citations))

    def _synthesize_rainfall_crop_comparison(self, all_dfs):
        response_part = ""
        rain_analyzed = False
        crop_analyzed = False

        for res_id, df in all_dfs.items():
            rain_col = self._find_col(df, "rainfall")
            state_col = self._find_col(df, "state")
            year_col = self._find_col(df, "year")

            if rain_col and state_col and year_col:
                df[rain_col] = self._clean_numeric(df[rain_col])
                avg_rain = df.groupby(state_col)[rain_col].mean().reset_index()
                response_part += "**Average Annual Rainfall Analysis:**\n"
                for _, row in avg_rain.iterrows():
                    response_part += f"* **{row[state_col]}**: {row[rain_col]:.2f} mm (average)\n"
                rain_analyzed = True

            crop_col = self._find_col(df, "crop")
            prod_col = self._find_col(df, "production")

            if crop_col and prod_col and state_col:
                df[prod_col] = self._clean_numeric(df[prod_col])
                top_crops = df.groupby(state_col)[prod_col].sum().nlargest(3).reset_index()
                response_part += f"\n**Top Produced Crops (by total {prod_col}):**\n"
                for _, row in top_crops.iterrows():
                    response_part += f"* **{row[state_col]}**: {row[prod_col]:.2f} (units as per source)\n"
                crop_analyzed = True

        if not rain_analyzed and not crop_analyzed:
            return "I found data, but could not identify clear columns for both rainfall and crop production by state."

        return response_part

    def _synthesize_production_leader(self, all_dfs, question_low):
        response_part = ""
        analyzed = False

        target_state = None
        if "karnataka" in question_low: target_state = "karnataka"
        if "maharashtra" in question_low: target_state = "maharashtra"
        if "state_x" in question_low: target_state = "state_x"

        for res_id, df in all_dfs.items():
            prod_col = self._find_col(df, "production")
            dist_col = self._find_col(df, "district")
            crop_col = self._find_col(df, "crop")
            state_col = self._find_col(df, "state")

            if prod_col and dist_col and crop_col:
                df[prod_col] = self._clean_numeric(df[prod_col])

                df_to_analyze = df
                if target_state and state_col:
                    df_to_analyze = df[df[state_col].str.lower() == target_state]

                if df_to_analyze.empty:
                    continue

                highest = df_to_analyze.loc[df_to_analyze[prod_col].idxmax()]
                lowest = df_to_analyze.loc[df_to_analyze[prod_col].idxmin()]

                response_part += f"\n**Production Analysis (from {res_id}):**\n"
                response_part += f"* **Highest Production:** **{highest[dist_col]}** district produced **{highest[prod_col]}** of **{highest[crop_col]}**.\n"
                response_part += f"* **Lowest Production:** **{lowest[dist_col]}** district produced **{lowest[prod_col]}** of **{lowest[crop_col]}**.\n"
                analyzed = True

        if not analyzed:
            return "I found data, but could not identify clear columns for production, district, and crop."

        return response_part

    def _synthesize_trend(self, all_dfs, question_low):
        response_part = ""
        analyzed = False

        for res_id, df in all_dfs.items():
            prod_col = self._find_col(df, "production")
            year_col = self._find_col(df, "year")
            crop_col = self._find_col(df, "crop")

            if prod_col and year_col and crop_col:
                df[prod_col] = self._clean_numeric(df[prod_col])
                df[year_col] = df[year_col].astype(str).str.strip()

                trend = df.groupby(year_col)[prod_col].sum().reset_index().sort_values(by=year_col)

                if len(trend) > 1:
                    response_part += f"\n**Production Trend Analysis (from {res_id}):**\n"
                    response_part += f"{trend.to_markdown(index=False)}\n\n"

                    start_year, start_val = trend.iloc[0][year_col], trend.iloc[0][prod_col]
                    end_year, end_val = trend.iloc[-1][year_col], trend.iloc[-1][prod_col]

                    if pd.notna(start_val) and pd.notna(end_val) and start_val > 0:
                        change = ((end_val - start_val) / start_val) * 100
                        response_part += f"Overall trend from {start_year} to {end_year} shows a **{change:.2f}% change** in production.\n"
                    analyzed = True

        if not analyzed:
            return "I found data, but could not identify clear columns for production, year, and crop to analyze a trend."
        return response_part

st.set_page_config(layout="wide", page_title="Project Samarth")
st.title("ðŸ‡®ðŸ‡³ Project Samarth")
st.markdown("An intelligent Q&A system for India's agricultural economy and climate data, powered by `data.gov.in`.")

api_key_input = st.text_input(
    "Enter your data.gov.in API Key",
    type="password",
    help="You must generate a free API key from https://data.gov.in/ to use this app."
)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "citations" in message and message["citations"]:
            with st.expander("View Data Sources"):
                for citation in message["citations"]:
                    st.markdown(f"- {citation}")

if prompt := st.chat_input("Ask a question, e.g., 'Compare rainfall and crop production in Karnataka'"):
    if not api_key_input:
        st.error("Please enter your data.gov.in API key above to start.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            try:
                planner = QueryPlanner()
                api = DataGovAPI(api_key=api_key_input)
                llm = LocalLLM()

                with st.spinner("1/4: Analyzing question and creating data plan..."):
                    search_queries = planner.create_plan(prompt)
                    st.write(f"**Data Plan:** I will search for datasets related to: `{', '.join(search_queries)}`")

                with st.spinner(f"2/4: Searching data.gov.in for {len(search_queries)} queries..."):
                    all_found_datasets = []
                    for query in search_queries:
                        found = api.search(query)
                        all_found_datasets.extend(found)

                    unique_datasets = {d["resource_id"]: d for d in all_found_datasets}.values()

                    if not unique_datasets:
                        st.error("No relevant datasets were found on data.gov.in for your query.")
                        st.stop()

                    st.write(f"**Data Discovery:** Found {len(unique_datasets)} unique, relevant datasets.")
                    with st.expander("View discovered datasets"):
                        for d in unique_datasets:
                            st.markdown(f"**{d['title']}** ({d['ministry']})\n*ID: {d['resource_id']}*")

                with st.spinner(f"3/4: Fetching live data for {len(unique_datasets)} datasets... (This may take a moment)"):
                    data_context = {}
                    for i, dataset in enumerate(unique_datasets):
                        st.write(f"Fetching {i+1}/{len(unique_datasets)}: {dataset['title'][:50]}...")
                        df, metadata = api.fetch(dataset["resource_id"])
                        if not df.empty:
                            data_context[dataset["resource_id"]] = (df, metadata)

                    if not data_context:
                        st.error("Data was discovered, but I failed to fetch any records. This can be due to API issues or empty datasets.")
                        st.stop()

                    st.write(f"**Data Retrieval:** Successfully fetched and loaded {len(data_context)} datasets.")

                with st.spinner("4/4: Synthesizing answer using local reasoning engine..."):
                    final_answer, citations = llm.generate(prompt, data_context)
                    st.markdown(final_answer)
                    if citations:
                        with st.expander("View Data Sources"):
                            for citation in citations:
                                st.markdown(f"- {citation}")

                st.session_state.messages.append({"role": "assistant", "content": final_answer, "citations": citations})

            except Exception as e:
                st.error(f"An unexpected error occurred: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"I'm sorry, I ran into an error: {e}"})